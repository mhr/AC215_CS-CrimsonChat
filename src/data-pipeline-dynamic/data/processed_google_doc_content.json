{
    "https://docs.google.com/document/d/1qBfsiK-NNe_dMIsShMSiJe5_Qsc2tmYJMSVzbsMw0RI": {
        "text_content": "Title: Harvard Theory of Computing Seminar (Fall 2024)\n\nContent:\nHarvard Theory of Computing Seminar \nAbout: The Harvard Theory of Computing seminar is the primary seminar series of the Harvard Theory of Computation Group. \nTime and place: The meetings happen roughly every other Wednesday from 3:45-5pm. (Typically there are refreshments before the seminar from 3:15-3:45, outside the seminar room. The talks end at 4:45 with 4:45-5:00pm reserved for Q&A.) The meetings in Fall 2024 will be held in the Harvard SEC, room 3.301-3.303 (tentative)\nMailing list: To join the theory seminar mailing list, please request access to the Theory of Computing Google Group or email Emma at eduncan@seas.harvard.edu. \nCalendar: Here\u2019s a link to add our google calendar, or you can view it here. \nFall 2024 Organizers: Anurag Anshu, Sitan Chen, Allison Choat, Rebekah Keely, Aaron (Louie) Putterman, Jake Ruotolo, Emma Duncan\nUpcoming Talks:\n09/13/2024: Special seminar: Avi Wigderson (more details)\n09/18/2024: Postdoc day (more details)\n09/25/2024: Jason Li (more details)\n10/09/2024: Siqi Liu (more details)  \n10/17/2024: Special seminar: Claudia Goldin (more details)\n10/23/2024: Sinho Chewi (more details)\n11/06/2024: Joan Bruna (more details)\n11/20/2024: Abhradeep Thakurta (more details)\n12/04/2024: Jinyoung Park (more details)\n12/11/2024: Toniann Pitassi (more details)\nPast Talks:\n\nSpring 2024 (info):\n01/24/2024: Postdoc day (more details)\n02/07/2024: Pravesh Kothari (more details)\n02/21/2024: Jane Lange (more details)  \n03/06/2024: Raghu Meka (more details)\n04/03/2024: Yael Tauman Kalai (more details)\n04/17/2024: Mohsen Ghaffari (more details)\n04/24/2024: Sanjeev Khanna (more details)\nOlder seminars: Here\u2019s a partial list. \nSeptember 13, 2024: Avi Wigderson \nCo-hosted by the CS Lecture Series and the Theory of Computing Seminar Series\n\nTime: 3:45 - 5:00 p.m.\n\nLocation: Winokur Family Hall 1.321\n\nTitle: The Value of Errors in Proofs\n\nSpeaker: Avi Wigderson (Institute for Advanced Study, Princeton University)\n\nAbstract:\n\nA few years ago, a group of theoretical computer scientists posted a paper on the Arxiv with the strange-looking title \"MIP* = RE\", surprising and impacting not only complexity theory but also some areas of math and physics. Specifically,  it resolved, in the negative, the \"Connes' embedding conjecture\" in the area of von-Neumann algebras, and the \"Tsirelson problem\" in quantum information theory. It further connects Turing's seminal 1936 paper which defined algorithms, to Einstein's 1935 paper with Podolsky and Rosen which challenged quantum mechanics. You can find the paper here: https://arxiv.org/abs/2001.04383.\n\nAs it happens, both acronyms MIP* and RE represent proof systems, of a very different nature. To explain them, we'll take a meandering journey through the classical and modern definitions of proof. I hope to explain how the methodology of computational complexity theory, especially modeling and classification (of both problems and proofs) by algorithmic efficiency, naturally leads to the generation of new such notions and results (and more acronyms, like NP). A special focus will be on notions of proof which allow interaction, randomness, and errors, and their surprising power and magical properties.\nSeptember 18, 2024: Postdoc Day\n\nTime: 3:45 - 5:00 p.m.\n\nLocation: SEC 2.122 + 2.123\n\nTitle:  Postdocs Day\n\nSpeakers: Lunjia Hu, Han Shao, Avi Karchmer, Nikhil Vyas\n\nAbstracts:\n\nLunjia Hu: Calibration Error for Decision Making\nWhere predictions gain value by assisting decision making, calibration allows predictions to\u000bbe reliably interpreted as probabilities by the decision makers. We propose a decision-theoretic\u000bcalibration error, the Calibration Decision Loss (CDL), defined as the maximum improvement\u000bin decision payoff obtained by calibrating the predictions, where the maximum is over all payoff- bounded decision tasks. Vanishing CDL guarantees the payoff loss from miscalibration vanishes simultaneously for all downstream decision tasks. We show a separation between CDL and existing calibration error metrics, including the most well-studied metric Expected Calibration Error (ECE). Our main technical contribution is a new algorithm for online calibration that achieves O(sqrt(T)log T) expected CDL, bypassing the \u03a9(T^{0.528}) lower bound for ECE by Qiao and Valiant (2021).\nJoint work with Yifan Wu. To appear in FOCS 2024.\n\nHan Shao: Learning From Strategic Data Sources\n\nIn contrast with standard classification tasks, strategic classification involves agents strategically modifying their features in an effort to receive favorable predictions. For instance, given a classifier determining loan approval based on credit scores, applicants may open or close their credit cards to fool the classifier. The learning goal is to find a classifier robust against strategic manipulations. We study the fundamental mistake bound and sample complexity in the strategic classification.\n\n\nAvi Karchmer: Cryptography and Complexity Theory in the Design and Analysis of Machine Learning\n\nCryptography and Complexity vs. Machine Learning---generally, the former two are in opposition to the latter. But when life gives you lemons... make lemonade! \n\u25cf Cryptography can help us design more secure and private ML algorithms\n\u25cf We may \u201cmine\u201d technical machinery from Complexity theory for faster and more robust ML algorithms\n\u25cf Both Cryptography and Complexity can help us reason about the ML \u201creal world\u201d (e.g., when is training on text and images more effective than training on just text?)\nThis \"Preview\" talk will highlight to the audience various situations in which Cryptography and Complexity theory can positively impact the design and analysis of Machine Learning. \n\n\nNikhil Vyas: Understanding and Improving the Shampoo Optimizer\nShampoo, a second-order optimization algorithm utilizing a Kronecker product preconditioner, has recently gained attention within the machine learning community. We show how Shampoo can be interpreted as a Kronecker-factored approximation of Newton's method. Building on this perspective, we explore modifications to Shampoo that lead to enhanced performance.\u000b Based on works with coauthors David Brandfonbrener, Lucas Janson, Sham Kakade, Eran Malach, Depen Morwani, Itai Shapira, and Rosie Zhao.\n\n\u000b\nSeptember 25, 2024: Jason Li (Carnegie Mellon University)\n\nTime: 3:45 - 5:00 p.m.\n\nLocation: SEC 3.301-3.303 \n\nTitle: Minimum Isolating Cuts: A new tool for solving minimum cut problems\n\nSpeaker: Jason Li (Carnegie Mellon University)\nJason Li is an assistant professor at CMU working on fundamental graph optimization problems such as minimum cut and maximum flow. His research efforts have led to state-of-the-art algorithms for a wide array of classic problems, including deterministic global minimum cut, minimum k-way cut, Gomory-Hu tree or all-pairs minimum cut, and single-source shortest path in parallel.\n\nAbstract:\nMinimum cut problems are among the most well-studied questions in combinatorial optimization. In this talk, I will introduce a simple but powerful new tool for solving minimum cut problems called the minimum isolating cuts. I will show how this tool can be employed to obtain faster algorithms for several fundamental min-cut problems, namely global min-cut, Steiner min-cut, and all-pairs min-cut. For these problems, the new results represent the first improvement in their runtimes in several decades.\nThese results are in collaboration with Amir Abboud, Robert Krauthgamer, Danupon Nanongkai, Thatchaphol Saranurak, and Ohad Trabelsi.\n\n\n\n\n\n\n\nOctober 9, 2024: Siqi Liu (DIMACS/IAS)\n\nTime: 3:45 - 5:00 p.m.\n\nLocation: SEC 3.301-3.303\n\nTitle:  Sparse spectral and coboundary high dimensional expanders\n\nSpeaker: Siqi Liu (DIMACS/IAS)\n\n\nSiqi Liu is a postdoc at the Institute for Advanced Study hosted by Avi Wigderson. She is generally interested in theoretical computer science. Most recently she is interested in graphs and their applications to coding theory, property testing, and complexity. \n\nAbstract: \nHigh dimensional expanders (HDXs) are a class of expanding hypergraphs. Their importance is constantly growing with their roles in the breakthroughs in Markov chains [ALGV19], locally testable codes [DELLM22], and quantum codes [PP22, ABN23]. HDXs generalize the notion of expander graphs but differ from the latter in a couple ways. First, while various definitions of expansion are equivalent over graphs, they are different in hypergraphs, yielding different types of HDXs. Secondly, while sparse expanders are abundant in random graph models, bounded-degree HDXs seem to be rare in random hypergraph models. \nSo far the sparest random construction of HDXs comes from random geometric graphs (RGGs) [LMSY23]. In this talk we will introduce a deterministic construction of HDXs which can be viewed as a derandomization of the RGG construction. These hypergraphs are both spectral HDXs and coboundary HDXs. They are the sparsest known coboundary HDXs (over any group). In particular, their 1-skeletons are abelian Cayley graphs.\nThis is based on joint work with Yotam Dikstein and Avi Wigderson.\n\nOctober 17, 2024: Claudia Goldin \n\nTime: 4:30pm- 5:30 pm\n\nLocation: SEC LL2.224\n\nTitle:  Why Women Won\n\nSpeaker: Claudia Goldin\n\nClaudia Goldin is the Henry Lee Professor of Economics at Harvard University and the 2023 Nobel Laureate in Economic Sciences. She was the director of the NBER\u2019s Development of the American Economy program from 1989 to 2017 and is currently co-director of the NBER\u2019s Gender in the Economy group. Most of her research interprets the present through the lens of the past and explores the origins of issues of current concern. She is the author of many books including: Understanding the Gender Gap: An Economic History of American Women, The Race between Education and Technology, and Career & Family: Women\u2019s Century-Long Journey Toward Equity. Goldin was the president of the American Economic Association and of the Economic History Association.  She is a member of the National Academy of Sciences and the American Philosophical Society. She is a fellow of the American Academy of Political and Social Science, the American Academy of Arts and Sciences, and the Society of Labor Economists. She received the Nobel Prize in Economics in 2023, the Nemmers Prize in Economics in 2020 and the BBVA Prize in 2019. She received her BA from Cornell University and her PhD from the University of Chicago. She is an outdoor enthusiast, a bird-watcher, and has trained Pika, her Golden Retriever, to several performance scenting titles.\n\nAbstract: \nHow, when, and why did women in the US obtain legal rights equal to men\u2019s regarding the workplace, marriage, family, Social Security, criminal justice, credit markets, and other parts of the economy and society, decades after they gained the right to vote? The story begins with the civil rights movement and the somewhat fortuitous nature of the early and key women\u2019s rights legislation. The women\u2019s movement formed and pressed for further rights. Of the 155 critical moments in women\u2019s rights history, I\u2019ve compiled from 1905 to 2023, 45% occurred between 1963 and 1973. The greatly increased employment of women, the formation of women\u2019s rights associations, the belief that women\u2019s votes mattered, and the unstinting efforts of various members of Congress were behind the advances. But women soon became splintered by marital status, employment, region, and religion far more than men. A substantial group of women emerged in the 1970s to oppose various rights for women, just as they did during the suffrage movement. They remain a potent force today.\n\n\nOctober 23, 2024: Sinho Chewi (Yale University)\n\nTime: 3:45 - 5:00 pm \n\nLocation: SEC 3.301-3.303\n\nTitle: Mean-field Langevin dynamics and two-layer neural networks\n\nSpeaker: Sinho Chewi\n\nAbstract: \n\nI will discuss recent progress on the theory of two-layer neural networks via the mean-field Langevin dynamics. In particular, I will discuss two key aspects of the finite-particle approximation to these dynamics, i.e., noisy gradient descent optimization of 2-layer neural networks: (1) how close these dynamics are to the mean-field limit, quantified by propagation of chaos, and (2) the rate of convergence to stationarity, quantified by a new log-Sobolev inequality that is independent of the number of particles. This is based on joint work with Murat A. Erdogdu, Yunbum Kook, Mufan (Bill) Li, Atsushi Nitanda, and Matthew S. Zhang.\nNovember 6, 2024: Joan Bruna Estrach (NYU)\n\nTime: 3:45 - 5:00 pm \n\nLocation: SEC 3.301-3.303\n\nTitle: Posterior Sampling with Denoising Oracles via Tilted Transport\n\nSpeaker: Joan Bruna Estrach\nJoan Bruna is a Professor of Computer Science, Data Science and Mathematics (aff) at the Courant Institute and the Center of Data Science, New York University. He is also a visiting scholar at the Flatiron Institute. His research interests are in the mathematical aspects of Machine Learning and their applications to computational science. For his research contributions, he has been awarded the Sloan Fellowship, the NSF Career award, and several best paper awards. \n\nAbstract:\nScore-based diffusion models have significantly advanced high-dimensional data generation across various domains, by learning a denoising oracle (or score) from datasets, providing an efficient sampling scheme going beyond typical isoperimetric assumptions. From a Bayesian perspective, they offer a realistic modeling of data priors, used for solving inverse problems through posterior sampling. Although many heuristic methods have been developed recently for this purpose, they lack the quantitative guarantees needed in many scientific applications.\nIn this talk, we introduce the tilted transport technique, which leverages the quadratic structure of the log-likelihood in linear inverse problems in combination with the prior denoising oracle to transform the original posterior sampling problem into a new `boosted' posterior that is provably easier to sample from. We quantify the conditions under which this boosted posterior is strongly log-concave, highlighting the dependencies on the condition number of the measurement matrix and the signal-to-noise ratio. The resulting posterior sampling scheme is shown to reach the computational threshold predicted for sampling Ising models [Kunisky'23] with a direct analysis, and is further validated on high-dimensional Gaussian mixture models and scalar field phi-4 models.\nJoint work with Jiequn Han (Flatiron Institute).\n\nNovember 20, 2024: Abhradeep Thakurta\n\nTime: 3:45 - 5:00 pm \n\nLocation: SEC 3.301-3.303\n\nTitle: Large Scale Private Learning on Data Streams, and the BLTs\n\nSpeaker: Abhradeep Thakurta (Google DeepMind)\n\nAbhradeep Guha Thakurta is a staff research scientist in Google DeepMind (formerly Google Research - Brain Team). His primary research interest is in the intersection of data privacy and machine learning. He focuses on demonstrating, both theoretically and in practice, that it is possible to design differentially private learning algorithms that can scale to industrial workloads. Prior to Google, Abhradeep was a faculty at UC Santa Cruz. And even before that he worked at Apple and Yahoo Labs as research scientists. He did his Ph.D. from The Pennsylvania State University in 2013, and his postdoctoral appointment was with Stanford University and Microsoft Research Silicon Valley Campus.\n\nAbstract: \nIn recent years, Differentially Private Follow the Regularized Leader (DP-FTRL) style algorithms have become increasingly popular for training large models over data streams, e.g., in training DP models for Gboard next-word prediction (https://research.google/blog/federated-learning-with-formal-differential-privacy-guarantees/ and https://arxiv.org/abs/2305.18465). At the heart of these algorithms is a correlated noise addition mechanism that factorizes a square lower triangular matrix of all ones (A=BC) and adds noise BZ (with Z ~ N(0,\u03c3^2)^{training steps x dimensions}) to the prefix sum of the gradients observed during training. Prior work required generating and storing the noise matrix in advance, which is prohibitively expensive for large-scale model training.\nIn this talk, we introduce a new algorithmic construction called the Buffered Linear Toeplitz operator (BLT) that allows generating the noise required for DP in a streaming fashion while requiring a buffer size of approximately log^2(training steps) x dimensions. Furthermore, compared to the class of all Lower Triangular Toeplitz (LTT) factorizations (i.e., B and C being LTT), BLTs are only off by an additive constant in terms of utility under standard error metrics. The talk will draw ideas from rational function approximations and constant recurrence sequences for the construction of BLTs. The talk will also demonstrate the practicality of this work through simulation experiments.\n\nThe talk is primarily based on https://arxiv.org/pdf/2404.16706 (to appear in FOCS 2024), and is a joint work with Krishnamurthy (Dj) Dvijotham, Brendan McMahan, Krishna Pillutla, and Thomas Steinke.\n\n\nDecember 4, 2024: Jinyoung Park\n\nTime: 3:45 - 5:00 pm \n\nLocation: SEC 3.301-3.303\n\nTitle: TBD\n\nSpeaker: Jinyoung Park (NYU)\n\nAbstract: TBD\nDecember 11, 2024: Toniann Pitassi\n\nTime: 3:45 - 5:00 pm \n\nLocation: SEC 3.301-3.303\n\nTitle: TBD\n\nSpeaker: Toniann Pitassi\n\nAbstract: TBD\n\nHarvard Theory of Computing Seminar (Spring \u201824)\nAbout: The Harvard Theory of Computing seminar is the primary seminar series of the Harvard Theory of Computation Group. \nTime and place: The meetings happen roughly every other Wednesday from 3:45-5pm. (Typically there are refreshments before the seminar from 3:15-3:45, outside the seminar room. The talks end at 4:45 with 4:45-5:00pm reserved for Q&A.) The meetings in Spring 2024 will be held in the Harvard SEC, room 3.301-3.303.\nMailing list: To join the theory seminar mailing list, please email Allison at achoat@g.harvard.edu.\nCalendar: Here\u2019s a link to add our google calendar, or you can view it here. \nOrganizers: Sumaira Ahammed, Anurag Anshu, Allison Choat, Rebekah Keely, Aaron (Louie) Putterman, Jake Ruotolo, Madhu Sudan.\nJanuary 24, 2024: Postdoc Day\n\nTime: 3:45 - 5:00 p.m.\n\nLocation: SEC 3.301-3.303 \n\nTitle:  Postdocs Day\n\nSpeakers: Vikrant Singhal, Tomer Ezra, Eran Malach\n\nAbstracts:\n\n\n\nFebruary 7, 2024: Pravesh Kothari\n\nTime: 3:45 - 5:00 p.m.\n\nLocation: SEC 3.301-3.303 \n\nTitle: Efficiently Finding Planted Cliques in Semirandom Graphs\n\nSpeaker: Pravesh Kothari\n\nAbstract: In this talk, I will present a new polynomial time algorithm for recovering planted cliques in the semi-random graph model of Feige and Kilian from 2001. In the Feige-Kilian model, a graph G on vertex set V is chosen by a combination of benign random and adaptive \"adversarial\" choices and can be thought of as a \"robust\" counterpart of the well-studied \"fully-random\" planted clique problem:\n1) choose a planted clique on a set of k vertices S, \n2) \"benign random choice\": include each edge between S and V\\S independently at random, \n3) \"adaptive worst-case choice\": deleting any set of edges between S and V\\S and adding any set of edges with both endpoints in V\\S. \n\nThe previous best algorithms for this model succeed if the planted clique has a size of at least n^{2/3} in a graph with n vertices. Our algorithms work for planted-clique sizes approaching n^{1/2} -- the information-theoretic threshold in the semi-random model and a conjectured computational threshold even in the easier fully-random model. This result comes close to resolving an open question of Feige (2019) and Steinhardt (2017). Our algorithm is based on high constant degree sum-of-squares relaxation of the clique SDP and the analysis relies on a new connection between finding planted cliques and efficient certificates of upper bounds on clique number of unbalanced bicliques in bipartite random graphs. \n\nBased on joint work with Rares Buhai and David Steurer. \n\n\n\n\n\n\nFebruary 21, 2024: Jane Lange\n\nTime: 3:45 - 5:00 p.m.\n\nLocation: SEC 3.301-3.303\n\nTitle:  Agnostic Proper Learning of Monotone Functions: Beyond the Black-Box Correction Barrier\n\nSpeaker: Jane Lange \n\nAbstract: We give the first agnostic, efficient, proper learning algorithm for monotone Boolean functions. Given 2^\u00d5(sqrt(n)/\u03b5) uniformly random examples of an unknown function f:{\u00b11}^n\u2192{\u00b11}, our algorithm outputs a hypothesis g:{\u00b11}^n\u2192{\u00b11} that is monotone and (opt+\u03b5)-close to f, where opt is the distance from f to the closest monotone function. The running time of the algorithm (and consequently the size and evaluation time of the hypothesis) is also 2^\u00d5(sqrt(n)/\u03b5), nearly matching the lower bound of Blais et al (RANDOM '15). We also give an algorithm for estimating up to additive error \u03b5 the distance of an unknown function f to monotone using a run-time of 2^\u00d5(sqrt(n)/\u03b5). Previously, for both of these problems, sample-efficient algorithms were known, but these algorithms were not run-time efficient. Our work thus closes this gap in our knowledge between the run-time and sample complexity. This work builds upon the improper learning algorithm of Bshouty and Tamon (JACM '96) and the proper semiagnostic learning algorithm of Lange, Rubinfeld, and Vasilyan (FOCS '22), which obtains a non-monotone Boolean-valued hypothesis, then \"corrects\" it to monotone using query-efficient local computation algorithms on graphs. This black-box correction approach can achieve no error better than 2opt+\u03b5 information-theoretically; we bypass this barrier by a) augmenting the improper learner with a convex optimization step, and b) learning and correcting a real-valued function before rounding its values to Boolean. Our real-valued correction algorithm solves the \"poset sorting'' problem of [LRV22] for functions over general posets with non-Boolean labels.\n\nMarch 6, 2024: Raghu Meka\n\nTime: 3:45 - 5:00 pm \n\nLocation: SEC 3.301-3.303\n\nTitle: New Frontiers in Structure vs Randomness with Applications to Combinatorics, Complexity, Algorithms\n\nSpeaker: Raghu Meka\n\nAbstract: In 1936, Erdos and Hajnal asked the following: Suppose you have a set S of integers from {1,2,..., N} that contains at least N / C elements. Then, for large enough N, must S have three equally spaced numbers (i.e., a 3-term arithmetic progression)? In 1946, Behrend showed that C can be at most exp(sqrt(log N)). Since then, the problem has been a cornerstone of the area of additive combinatorics, with the best bound being C = (log N)^(1+c) for some constant c > 0. Recent work obtained an exponential improvement showing that C can be as big as exp((log N)^0.09), thus getting closer to Behrend's construction. \n\nIn this talk, I will describe this result and the main ingredient, a new variant of the \"structure vs. randomness\" paradigm. The latter is an old technique with many applications in complexity theory, algorithm design, and number theory, and the new variant can potentially lead to further progress. I will highlight two such applications: \n1. Communication complexity: explicit separations between randomized and deterministic multi-party protocols.\n2. Algorithm design: fast combinatorial algorithms for Boolean matrix multiplication, detecting triangles in graphs. \n\nBased on works with Amir Abboud, Nick Fischer, Zander Kelley, Shachar Lovett.\n\n\nApril 3, 2024: Yael Tauman Kalai\n\nTime: 3:45 - 5:00 pm \n\nLocation: SEC 3.301-3.303\n\nTitle:  Classical Commitments to Quantum States and Application to Succinct Verification of QMA\n\nSpeaker: Yael Tauman Kalai\n\nAbstract: We define the notion of a classical commitment scheme to quantum states, which allows a quantum prover to compute a classical commitment to a quantum state, and later open each qubit of the state in either the standard or the Hadamard basis.  Our notion is a strengthening of the measurement protocol from  Mahadev (STOC 2018).  We construct such a commitment scheme from the post-quantum Learning With Errors (LWE) assumption. Our scheme is succinct in the sense that the running time of the verifier in the commitment phase depends only on the security parameter (independent of the size of the committed state), and its running time in the opening phase grows only with the number of qubits that are being opened (and the security parameter).  As a corollary we obtain a classical succinct argument system for QMA under the post-quantum LWE assumption.  Previously, this was only known assuming post-quantum secure indistinguishability obfuscation.  As an additional corollary we obtain a generic way of converting any X/Z quantum PCP into a succinct argument system under the quantum hardness of LWE.  This is joint work with Sam Gunn, Anand Natarajan and Agi Villanyi.\n\n\n\nApril 17, 2024: Mohsen Ghaffari\n\nTime: 3:45 - 5:00 pm \n\nLocation: SEC 3.301-3.303\n\nTitle:  Parallel Derandomization for Chernoff-like Concentrations\n\nSpeaker: Mohsen Ghaffari\n\nAbstract:TBD \nApril 24, 2024: Sanjeev Khanna\n\nTime: 3:45 - 5:00 pm \n\nLocation: SEC 3.301-3.303\n\nTitle:  TBD\n\nSpeaker: Sanjeev Khanna\n\nAbstract:TBD \n\n",
        "metadata": {
            "last_modified": "2024-10-23T13:52:24Z",
            "scraped_at": "2024-11-12T07:30:23Z",
            "word_count": 3718,
            "url": "https://docs.google.com/document/d/1qBfsiK-NNe_dMIsShMSiJe5_Qsc2tmYJMSVzbsMw0RI"
        }
    }
}